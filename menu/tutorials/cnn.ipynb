{
 "cells": [
  {
   "cell_type": "raw",
   "id": "10f63e83",
   "metadata": {},
   "source": [
    "---\n",
    "title: CNN Tutorial\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "        page-layout: full\n",
    "        theme: flatly\n",
    "\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# Convolutional SNN\n",
    "### **Classifying Fashion-MNIST with Convolutional SNN**\n",
    "\n",
    "This tutorial goes over how to train a convolutional spiking neural network (CSNN) on the Fashion-MNIST dataset and deploy on HiAER Spike using our conversion pipline.\n",
    "\n",
    "### **Define a CSNN**\n",
    "To build a CSNN with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c1238a-bb88-4a0e-9250-2ab4656dfaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.12.1)\n",
      "Requirement already satisfied: matplotlib in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (3.4.3)\n",
      "Requirement already satisfied: numpy in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (4.63.0)\n",
      "Requirement already satisfied: torchvision in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (0.13.1)\n",
      "Requirement already satisfied: scipy in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (9.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (2.8.2)\n",
      "Requirement already satisfied: typing_extensions in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from torch->spikingjelly) (4.3.0)\n",
      "Requirement already satisfied: requests in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from torchvision->spikingjelly) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spikingjelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a CSNN with the architecture of 8C3-BN-6272FC10\n",
    "- 8C3: a 3x3 convolutional kernel with 8 channels\n",
    "- BN: batch normalization layer \n",
    "- 6272FC10: the fully connected output layer \n",
    " \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, channels=8): \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        self.lif1 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear = nn.Linear(channels * 28 * 28, 10, bias=False)\n",
    "        self.lif2 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.lif1(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.lif2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Network\n",
    "net = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd241796-eebd-4400-8dc0-8d1eb177a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download Fashion-MNIST data from torch \n",
    "fashion_mnist_train = datasets.FashionMNIST('data/fashion_mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "fashion_mnist_test = datasets.FashionMNIST('data/fashion_mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 40\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, train_loss = 0.0357, train_acc = 0.7461, test_loss = 0.0235, test_acc = 0.8520\n",
      "train speed = 2935.7557 images/s, test speed = 5597.1990 images/s\n",
      "epoch = 1, train_loss = 0.0207, train_acc = 0.8726, test_loss = 0.0210, test_acc = 0.8689\n",
      "train speed = 3042.4015 images/s, test speed = 7519.0884 images/s\n",
      "epoch = 2, train_loss = 0.0186, train_acc = 0.8858, test_loss = 0.0196, test_acc = 0.8771\n",
      "train speed = 2991.0198 images/s, test speed = 7575.3957 images/s\n",
      "epoch = 3, train_loss = 0.0174, train_acc = 0.8933, test_loss = 0.0194, test_acc = 0.8791\n",
      "train speed = 2984.4427 images/s, test speed = 7826.6419 images/s\n",
      "epoch = 4, train_loss = 0.0166, train_acc = 0.8992, test_loss = 0.0188, test_acc = 0.8817\n",
      "train speed = 2894.7927 images/s, test speed = 7270.1850 images/s\n",
      "epoch = 5, train_loss = 0.0161, train_acc = 0.9024, test_loss = 0.0186, test_acc = 0.8845\n",
      "train speed = 2872.4032 images/s, test speed = 7255.2524 images/s\n",
      "epoch = 6, train_loss = 0.0156, train_acc = 0.9071, test_loss = 0.0183, test_acc = 0.8843\n",
      "train speed = 2864.3767 images/s, test speed = 7292.0461 images/s\n",
      "epoch = 7, train_loss = 0.0151, train_acc = 0.9092, test_loss = 0.0181, test_acc = 0.8857\n",
      "train speed = 2910.3491 images/s, test speed = 7335.5046 images/s\n",
      "epoch = 8, train_loss = 0.0148, train_acc = 0.9122, test_loss = 0.0181, test_acc = 0.8865\n",
      "train speed = 2835.3971 images/s, test speed = 7393.1889 images/s\n",
      "epoch = 9, train_loss = 0.0144, train_acc = 0.9149, test_loss = 0.0181, test_acc = 0.8866\n",
      "train speed = 2879.8269 images/s, test speed = 7255.6936 images/s\n",
      "epoch = 10, train_loss = 0.0141, train_acc = 0.9172, test_loss = 0.0175, test_acc = 0.8887\n",
      "train speed = 2916.7074 images/s, test speed = 7257.2076 images/s\n",
      "epoch = 11, train_loss = 0.0139, train_acc = 0.9185, test_loss = 0.0175, test_acc = 0.8917\n",
      "train speed = 2881.1992 images/s, test speed = 7267.8002 images/s\n",
      "epoch = 12, train_loss = 0.0136, train_acc = 0.9194, test_loss = 0.0174, test_acc = 0.8913\n",
      "train speed = 2868.4384 images/s, test speed = 7186.9973 images/s\n",
      "epoch = 13, train_loss = 0.0135, train_acc = 0.9212, test_loss = 0.0172, test_acc = 0.8929\n",
      "train speed = 2847.0029 images/s, test speed = 7292.8589 images/s\n",
      "epoch = 14, train_loss = 0.0132, train_acc = 0.9226, test_loss = 0.0172, test_acc = 0.8909\n",
      "train speed = 2937.0686 images/s, test speed = 7250.2642 images/s\n",
      "epoch = 15, train_loss = 0.0130, train_acc = 0.9251, test_loss = 0.0171, test_acc = 0.8895\n",
      "train speed = 2859.6746 images/s, test speed = 7372.2077 images/s\n",
      "epoch = 16, train_loss = 0.0129, train_acc = 0.9248, test_loss = 0.0170, test_acc = 0.8941\n",
      "train speed = 2865.2495 images/s, test speed = 7335.4879 images/s\n",
      "epoch = 17, train_loss = 0.0129, train_acc = 0.9251, test_loss = 0.0170, test_acc = 0.8940\n",
      "train speed = 2966.7517 images/s, test speed = 7922.5248 images/s\n",
      "epoch = 18, train_loss = 0.0127, train_acc = 0.9265, test_loss = 0.0170, test_acc = 0.8963\n",
      "train speed = 3061.9508 images/s, test speed = 7756.4266 images/s\n",
      "epoch = 19, train_loss = 0.0128, train_acc = 0.9263, test_loss = 0.0169, test_acc = 0.8908\n",
      "train speed = 3096.0437 images/s, test speed = 7756.2686 images/s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(num_steps):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_fr = out_fr/num_steps  \n",
    "        loss = loss_fun(out_fr, label_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr/num_steps  \n",
    "\n",
    "            loss = loss_fun(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "\n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized:  conv\n",
      "Quantized:  bn\n",
      "Quantized:  lif1\n",
      "Quantized:  flat\n",
      "Quantized:  linear\n",
      "Quantized:  lif2\n",
      "Quantization time: 0.0012586116790771484\n",
      "Constructing Axons from Conv2d Layer\n",
      "Input layer shape(infeature, outfeature): [ 1 28 28] [ 8 28 28]\n",
      "Input.shape: (28, 28)\n",
      "input_padded: (1, 30, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 8 bias axons from conv layer.\n",
      "Numer of neurons: 0, number of axons: 792\n",
      "Converting Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) takes 0.33516740798950195\n",
      "Constructing neurons from linear Layer\n",
      "Hidden layer shape(infeature, outfeature):  (8, 28, 28) 10\n",
      "Instantiate output neurons\n",
      "Numer of neurons: 6282, number of axons: 792\n"
     ]
    }
   ],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network, BN_Folder\n",
    "from hs_api.api import CRI_network\n",
    "# import hs_bridge #Uncomment when running on FPGA\n",
    "\n",
    "#Fold the BN layer \n",
    "bn = BN_Folder() \n",
    "net_bn = bn.fold(net)\n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net_bn)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 0 #first pytorch layer that acts as synapses\n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(quan_fun.v_threshold)\n",
    "    \n",
    "# #Uncomment this to create a network running on the FPGA\n",
    "# hardwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "#                               connections=dict(cri_convert.neuron_dict),\n",
    "#                               config=config,target='CRI', \n",
    "#                               outputs = cri_convert.output_neurons,\n",
    "#                               coreID=1)\n",
    "\n",
    "softwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "                              connections=dict(cri_convert.neuron_dict),\n",
    "                              config=config,target='simpleSim', \n",
    "                              outputs = cri_convert.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "run_sw and run_hw are two helper functions for running the spiking neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc80f35-b960-49c2-9631-b7981cf23d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_sw(self,inputList,softwareNetwork):\n",
    "    predictions = []\n",
    "    total_time_cri = 0\n",
    "    #each image\n",
    "    for currInput in tqdm(inputList):\n",
    "        #reset the membrane potential to zero\n",
    "        softwareNetwork.simpleSim.initialize_sim_vars(len(self.neuron_dict))\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            swSpike = softwareNetwork.step(slice, membranePotential=False)\n",
    "\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in swSpike:\n",
    "                spikeIdx = int(spike) - self.bias_start_idx \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate)\n",
    "        predictions.append(spikeRate.index(max(spikeRate)))\n",
    "    print(f\"Total simulation execution time: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669d786-cbab-4b5c-9b4c-ab2db0baeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI_hw(self,inputList,hardwareNetwork):\n",
    "    predictions = []\n",
    "    #each image\n",
    "    total_time_cri = 0\n",
    "    for currInput in tqdm(inputList):\n",
    "        #initiate the hardware for each image\n",
    "        hs_bridge.FPGA_Execution.fpga_controller.clear(len(self.neuron_dict), False, 0)  ##Num_neurons, simDump, coreOverride\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in tqdm(currInput):\n",
    "            start_time = time.time()\n",
    "            hwSpike, latency, hbmAcc = hardwareNetwork.step(slice, membranePotential=False)\n",
    "            print(f'hwSpike: {hwSpike}\\n. latency : {latency}\\n. hbmAcc:{hbmAcc}')\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in hwSpike:\n",
    "                # print(int(spike))\n",
    "                spikeIdx = int(spike) - self.bias_start_idx \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate)\n",
    "        predictions.append(spikeRate.index(max(spikeRate))) \n",
    "    print(f\"Total execution time CRIFPGA: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cri_convert.bias_start_idx = int(cri_convert.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "RUN_HARDWARE = False #Set to True if running on FPGA\n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cri_convert.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
