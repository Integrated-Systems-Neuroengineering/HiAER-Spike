{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# Convolutional SNN\n",
    "### **Classifying Fashion-MNIST with Convolutional SNN**\n",
    "\n",
    "This tutorial goes over how to train a convolutional spiking neural network (CSNN) on the Fashion-MNIST dataset and deploy on HiAER Spike using our conversion pipline.\n",
    "\n",
    "### **Define a CSNN**\n",
    "To build a CSNN with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c1238a-bb88-4a0e-9250-2ab4656dfaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (2.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (3.7.1)\n",
      "Requirement already satisfied: numpy in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (4.65.0)\n",
      "Requirement already satisfied: torchvision in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (0.15.2)\n",
      "Requirement already satisfied: scipy in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from spikingjelly) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib->spikingjelly) (2.8.2)\n",
      "Requirement already satisfied: filelock in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch->spikingjelly) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch->spikingjelly) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch->spikingjelly) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch->spikingjelly) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch->spikingjelly) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchvision->spikingjelly) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision->spikingjelly) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision->spikingjelly) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision->spikingjelly) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision->spikingjelly) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from sympy->torch->spikingjelly) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: torchviz in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: torchview in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: numpy in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: graphviz in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from torchviz) (0.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: hs_api in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: PyYAML<7.0,>=6.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (6.0)\n",
      "Requirement already satisfied: bidict<0.23.0,>=0.22.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (0.22.1)\n",
      "Requirement already satisfied: connectome-utils<0.2.0,>=0.1.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (0.1.0)\n",
      "Requirement already satisfied: fxpmath<0.5.0,>=0.4.8 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (0.4.8)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.5.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (3.7.1)\n",
      "Requirement already satisfied: numba<0.56.0,>=0.55.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (0.55.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (1.22.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from hs_api) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.5.1->hs_api) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from numba<0.56.0,>=0.55.1->hs_api) (0.38.1)\n",
      "Requirement already satisfied: setuptools in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from numba<0.56.0,>=0.55.1->hs_api) (67.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shashankbansal/miniconda3/envs/hsapi3_10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.5.1->hs_api) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spikingjelly\n",
    "!pip install torchvision torchviz torchview\n",
    "\n",
    "# Library to interact with the CRI hardware\n",
    "!pip install hs_api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a CSNN with the architecture of 8C3-BN-6272FC10\n",
    "- 8C3: a 3x3 convolutional kernel with 8 channels\n",
    "- BN: batch normalization layer \n",
    "- 6272FC10: the fully connected output layer \n",
    " \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, channels=8): \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        self.lif1 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear = nn.Linear(channels * 28 * 28, 10, bias=False)\n",
    "        self.lif2 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.lif1(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.lif2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(\n",
      "  (conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lif1): IFNode(\n",
      "    v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch\n",
      "    (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=6272, out_features=10, bias=False)\n",
      "  (lif2): IFNode(\n",
      "    v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch\n",
      "    (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initiate the Network\n",
    "net = model()\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(net)\n",
    "\n",
    "#TODO: print out model architecture as a graph\n",
    "# batch = next(iter(train_loader))\n",
    "# yhat = model(batch) # Give dummy batch to forward().\n",
    "\n",
    "# make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"SCNN\", format=\"png\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd241796-eebd-4400-8dc0-8d1eb177a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download Fashion-MNIST data from torch \n",
    "fashion_mnist_train = datasets.FashionMNIST('data/fashion_mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "fashion_mnist_test = datasets.FashionMNIST('data/fashion_mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbf11b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 40\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 20\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\") # if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, train_loss = 0.0271, train_acc = 0.8288, test_loss = 0.0232, test_acc = 0.8513\n",
      "train speed = 822.3998 images/s, test speed = 2005.7721 images/s\n",
      "epoch = 1, train_loss = 0.0206, train_acc = 0.8728, test_loss = 0.0212, test_acc = 0.8661\n",
      "train speed = 838.0144 images/s, test speed = 2085.9205 images/s\n",
      "epoch = 2, train_loss = 0.0189, train_acc = 0.8844, test_loss = 0.0204, test_acc = 0.8715\n",
      "train speed = 851.7008 images/s, test speed = 2122.0700 images/s\n",
      "epoch = 3, train_loss = 0.0178, train_acc = 0.8902, test_loss = 0.0196, test_acc = 0.8753\n",
      "train speed = 837.6941 images/s, test speed = 2034.1627 images/s\n",
      "epoch = 4, train_loss = 0.0171, train_acc = 0.8951, test_loss = 0.0192, test_acc = 0.8768\n",
      "train speed = 824.7459 images/s, test speed = 2091.6778 images/s\n",
      "epoch = 5, train_loss = 0.0165, train_acc = 0.8989, test_loss = 0.0192, test_acc = 0.8780\n",
      "train speed = 827.2133 images/s, test speed = 2051.0987 images/s\n",
      "epoch = 6, train_loss = 0.0159, train_acc = 0.9043, test_loss = 0.0191, test_acc = 0.8806\n",
      "train speed = 846.0593 images/s, test speed = 2138.0910 images/s\n",
      "epoch = 7, train_loss = 0.0155, train_acc = 0.9068, test_loss = 0.0187, test_acc = 0.8834\n",
      "train speed = 841.2864 images/s, test speed = 2117.1526 images/s\n",
      "epoch = 8, train_loss = 0.0151, train_acc = 0.9100, test_loss = 0.0184, test_acc = 0.8862\n",
      "train speed = 841.2309 images/s, test speed = 2050.5501 images/s\n",
      "epoch = 9, train_loss = 0.0148, train_acc = 0.9126, test_loss = 0.0186, test_acc = 0.8814\n",
      "train speed = 837.0163 images/s, test speed = 2123.6349 images/s\n",
      "epoch = 10, train_loss = 0.0145, train_acc = 0.9135, test_loss = 0.0179, test_acc = 0.8861\n",
      "train speed = 846.0572 images/s, test speed = 2073.8222 images/s\n",
      "epoch = 11, train_loss = 0.0142, train_acc = 0.9164, test_loss = 0.0180, test_acc = 0.8840\n",
      "train speed = 841.7076 images/s, test speed = 2115.7900 images/s\n",
      "epoch = 12, train_loss = 0.0140, train_acc = 0.9171, test_loss = 0.0175, test_acc = 0.8870\n",
      "train speed = 842.7662 images/s, test speed = 2102.5516 images/s\n",
      "epoch = 13, train_loss = 0.0138, train_acc = 0.9185, test_loss = 0.0172, test_acc = 0.8923\n",
      "train speed = 847.0846 images/s, test speed = 2109.5986 images/s\n",
      "epoch = 14, train_loss = 0.0136, train_acc = 0.9197, test_loss = 0.0174, test_acc = 0.8896\n",
      "train speed = 839.0720 images/s, test speed = 2155.3064 images/s\n",
      "epoch = 15, train_loss = 0.0134, train_acc = 0.9215, test_loss = 0.0175, test_acc = 0.8890\n",
      "train speed = 848.2634 images/s, test speed = 2117.8055 images/s\n",
      "epoch = 16, train_loss = 0.0133, train_acc = 0.9225, test_loss = 0.0173, test_acc = 0.8915\n",
      "train speed = 844.2177 images/s, test speed = 2104.1428 images/s\n",
      "epoch = 17, train_loss = 0.0132, train_acc = 0.9232, test_loss = 0.0174, test_acc = 0.8888\n",
      "train speed = 836.3772 images/s, test speed = 2019.0169 images/s\n",
      "epoch = 18, train_loss = 0.0131, train_acc = 0.9241, test_loss = 0.0173, test_acc = 0.8913\n",
      "train speed = 813.3396 images/s, test speed = 2101.0406 images/s\n",
      "epoch = 19, train_loss = 0.0130, train_acc = 0.9246, test_loss = 0.0172, test_acc = 0.8894\n",
      "train speed = 832.6740 images/s, test speed = 2064.9469 images/s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(num_steps):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_fr = out_fr/num_steps  \n",
    "        loss = loss_fun(out_fr, label_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr/num_steps  \n",
    "\n",
    "            loss = loss_fun(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    \n",
    "    \n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network, BN_Folder\n",
    "from hs_api.api import CRI_network\n",
    "# import hs_bridge #Uncomment when running on FPGA\n",
    "\n",
    "#Fold the BN layer \n",
    "bn = BN_Folder() \n",
    "net_bn = bn.fold(net)\n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net_bn)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 0 #first pytorch layer that acts as synapses\n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER-Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(quan_fun.v_threshold)\n",
    "    \n",
    "#Create a network running on the FPGA\n",
    "hardwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "                              connections=dict(cri_convert.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='CRI', \n",
    "                              outputs = cri_convert.output_neurons,\n",
    "                              coreID=1)\n",
    "\n",
    "#Create a network running on the software simulation\n",
    "softwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "                              connections=dict(cri_convert.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='simpleSim', \n",
    "                              outputs = cri_convert.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "Using the run_CRI_hw and run_CRI_sw method from the CRI_Converter class, we can deploy the converted SNN on the HiAER Spike platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cri_convert.bias_start_idx = int(cri_convert.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "RUN_HARDWARE = False #Set to True if running on FPGA\n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cri_convert.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(cri_convert.run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(cri_convert.run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
