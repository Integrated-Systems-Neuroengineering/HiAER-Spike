{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# Convolutional SNN\n",
    "### **Classifying Fashion-MNIST with Convolutional SNN**\n",
    "\n",
    "This tutorial goes over how to train a convolutional spiking neural network (CSNN) on the Fashion-MNIST dataset and deploy on HiAER Spike using our conversion pipline.\n",
    "\n",
    "### **Define a CSNN**\n",
    "To build a CSNN with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1238a-bb88-4a0e-9250-2ab4656dfaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spikingjelly\n",
    "!pip install torchvision torchviz torchview\n",
    "!pip install snntorch\n",
    "\n",
    "# Library to interact with the CRI hardware\n",
    "!pip install hs_api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a CSNN with the architecture of 8C3-BN-6272FC10\n",
    "- 8C3: a 3x3 convolutional kernel with 8 channels\n",
    "- BN: batch normalization layer \n",
    "- 6272FC10: the fully connected output layer \n",
    " \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, channels=8): \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        self.lif1 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear = nn.Linear(channels * 28 * 28, 10, bias=False)\n",
    "        self.lif2 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.lif1(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.lif2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Network\n",
    "net = model()\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(net)\n",
    "\n",
    "#TODO: print out model architecture as a graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd241796-eebd-4400-8dc0-8d1eb177a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download Fashion-MNIST data from torch \n",
    "fashion_mnist_train = datasets.FashionMNIST('data/fashion_mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "fashion_mnist_test = datasets.FashionMNIST('data/fashion_mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a543b5c6",
   "metadata": {},
   "source": [
    "### [Optional] : The following is for Apple Silicon (M1/M2) users only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 40\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 5\n",
    "dtype = torch.float\n",
    "\n",
    "# Use Apple metal accelerator for training\n",
    "device = torch.device(\"mps\") \n",
    "# Uncomment the following if you are not on apple silicon\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d5a51d6",
   "metadata": {},
   "source": [
    "### Train the CNN\n",
    "Depending on your device (GPU/M1/M2) it can take somewhere between 15-20 mins (longer on CPU). We recommmend using the pretrained-model for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        \n",
    "        # Move encoding outside the loop\n",
    "        encoded_img = encoder(img)\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            out_fr += net(encoded_img)\n",
    "        out_fr = out_fr/num_steps  \n",
    "        loss = loss_fun(out_fr, label_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr/num_steps  \n",
    "\n",
    "            loss = loss_fun(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    }, f\"pretrained-models/checkpoint_csnn_{device}.pth\")\n",
    "    \n",
    "    \n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pretrained-model\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!wget https://github.com/Integrated-Systems-Neuroengineering/hiaer-spike/raw/main/notebooks/pretrained-models/checkpoint_max_csnn.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d025f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"pretrained-models/checkpoint_csnn_mps.pth\", map_location=device)\n",
    "\n",
    "# # Load the state dictionaries\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "\n",
    "# # If you want to continue training from the saved epoch\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snntorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network, BN_Folder\n",
    "from hs_api.api import CRI_network\n",
    "\n",
    "#Fold the BN layer \n",
    "bn = BN_Folder() \n",
    "net_bn = bn.fold(net)\n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net_bn)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 0 #first pytorch layer that acts as synapses\n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER-Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(qn.v_threshold)\n",
    "    \n",
    "#Create a network running on the FPGA\n",
    "hardwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='CRI', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)\n",
    "\n",
    "#Create a network running on the software simulation\n",
    "softwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='simpleSim', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "Using the run_CRI_hw and run_CRI_sw method from the CRI_Converter class, we can deploy the converted SNN on the HiAER Spike platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cn.bias_start_idx = int(cn.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "RUN_HARDWARE = False #Set to True if running on FPGA\n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cn.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(cn.run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(cn.run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
