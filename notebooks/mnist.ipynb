{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6fb71424",
   "metadata": {},
   "source": [
    "---\n",
    "title: MNIST Tutorial\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "        page-layout: full\n",
    "        theme: flatly\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "### **Feedforward Fully Connected SNN**\n",
    "\n",
    "This tutorial goes over how to train a simple feedforward SNN and deploy on HiAER Spike using our \n",
    "conversion pipline.\n",
    "\n",
    "### **Define a Feedforward SNN**\n",
    "To build a simple feedforward spiking neural network with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c1238a-bb88-4a0e-9250-2ab4656dfaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.12.1)\n",
      "Requirement already satisfied: matplotlib in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (3.4.3)\n",
      "Requirement already satisfied: numpy in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (4.63.0)\n",
      "Requirement already satisfied: torchvision in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (0.13.1)\n",
      "Requirement already satisfied: scipy in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from spikingjelly) (1.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (9.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from matplotlib->spikingjelly) (2.8.2)\n",
      "Requirement already satisfied: typing_extensions in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from torch->spikingjelly) (4.3.0)\n",
      "Requirement already satisfied: requests in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from torchvision->spikingjelly) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages (from requests->torchvision->spikingjelly) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spikingjelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a simple 2-layer feedforward SNN model with 1000 hidden neurons. The PyTorch layer will act as synapses between the spiking neuron layers. \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, features = 1000): \n",
    "        super().__init__() \n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28 * 28, features, bias=False) \n",
    "        self.lif1 = neuron.LIFNode(surrogate_function=surrogate.ATan()) \n",
    "        self.linear2 = nn.Linear(features, 10, bias=False) \n",
    "        self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan()) \n",
    "    def forward(self, x): \n",
    "        x = self.flat(x)\n",
    "        x = self.linear1(x) \n",
    "        x = self.lif1(x) \n",
    "        x = self.linear2(x) \n",
    "        x = self.lif2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Network\n",
    "net = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd241796-eebd-4400-8dc0-8d1eb177a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download MNIST data from torch \n",
    "mnist_train = datasets.MNIST('data/mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "mnist_test = datasets.MNIST('data/mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 20\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, train_loss = 0.0188, train_acc = 0.8899, test_loss = 0.0082, test_acc = 0.9558\n",
      "train speed = 4051.3646 images/s, test speed = 6689.1926 images/s\n",
      "epoch = 1, train_loss = 0.0065, train_acc = 0.9668, test_loss = 0.0057, test_acc = 0.9707\n",
      "train speed = 4502.3399 images/s, test speed = 10131.3872 images/s\n",
      "epoch = 2, train_loss = 0.0044, train_acc = 0.9779, test_loss = 0.0046, test_acc = 0.9765\n",
      "train speed = 4497.2928 images/s, test speed = 9969.6456 images/s\n",
      "epoch = 3, train_loss = 0.0034, train_acc = 0.9841, test_loss = 0.0040, test_acc = 0.9791\n",
      "train speed = 4501.9602 images/s, test speed = 10136.5373 images/s\n",
      "epoch = 4, train_loss = 0.0027, train_acc = 0.9880, test_loss = 0.0035, test_acc = 0.9811\n",
      "train speed = 4473.6464 images/s, test speed = 10003.3661 images/s\n",
      "epoch = 5, train_loss = 0.0022, train_acc = 0.9909, test_loss = 0.0033, test_acc = 0.9831\n",
      "train speed = 4538.8954 images/s, test speed = 10411.7390 images/s\n",
      "epoch = 6, train_loss = 0.0018, train_acc = 0.9930, test_loss = 0.0033, test_acc = 0.9822\n",
      "train speed = 4602.6228 images/s, test speed = 11052.5053 images/s\n",
      "epoch = 7, train_loss = 0.0015, train_acc = 0.9948, test_loss = 0.0032, test_acc = 0.9833\n",
      "train speed = 4477.6290 images/s, test speed = 10118.0900 images/s\n",
      "epoch = 8, train_loss = 0.0013, train_acc = 0.9961, test_loss = 0.0030, test_acc = 0.9846\n",
      "train speed = 4499.3006 images/s, test speed = 10151.5785 images/s\n",
      "epoch = 9, train_loss = 0.0011, train_acc = 0.9966, test_loss = 0.0032, test_acc = 0.9839\n",
      "train speed = 4472.8697 images/s, test speed = 10166.8593 images/s\n",
      "epoch = 10, train_loss = 0.0009, train_acc = 0.9976, test_loss = 0.0029, test_acc = 0.9841\n",
      "train speed = 4498.0421 images/s, test speed = 9962.0585 images/s\n",
      "epoch = 11, train_loss = 0.0007, train_acc = 0.9982, test_loss = 0.0027, test_acc = 0.9853\n",
      "train speed = 4516.1326 images/s, test speed = 9973.3758 images/s\n",
      "epoch = 12, train_loss = 0.0006, train_acc = 0.9986, test_loss = 0.0028, test_acc = 0.9840\n",
      "train speed = 4458.5721 images/s, test speed = 10007.9276 images/s\n",
      "epoch = 13, train_loss = 0.0006, train_acc = 0.9988, test_loss = 0.0028, test_acc = 0.9844\n",
      "train speed = 4533.3190 images/s, test speed = 10187.7078 images/s\n",
      "epoch = 14, train_loss = 0.0005, train_acc = 0.9990, test_loss = 0.0027, test_acc = 0.9838\n",
      "train speed = 4523.5437 images/s, test speed = 9711.6715 images/s\n",
      "epoch = 15, train_loss = 0.0004, train_acc = 0.9991, test_loss = 0.0028, test_acc = 0.9846\n",
      "train speed = 4456.8744 images/s, test speed = 10176.8190 images/s\n",
      "epoch = 16, train_loss = 0.0004, train_acc = 0.9992, test_loss = 0.0027, test_acc = 0.9852\n",
      "train speed = 4695.8999 images/s, test speed = 9990.2977 images/s\n",
      "epoch = 17, train_loss = 0.0003, train_acc = 0.9993, test_loss = 0.0027, test_acc = 0.9858\n",
      "train speed = 4524.9890 images/s, test speed = 10020.5386 images/s\n",
      "epoch = 18, train_loss = 0.0003, train_acc = 0.9995, test_loss = 0.0026, test_acc = 0.9854\n",
      "train speed = 4551.8444 images/s, test speed = 9910.9680 images/s\n",
      "epoch = 19, train_loss = 0.0003, train_acc = 0.9995, test_loss = 0.0027, test_acc = 0.9851\n",
      "train speed = 4524.7967 images/s, test speed = 10098.8693 images/s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(num_steps):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_fr = out_fr/num_steps  \n",
    "        loss = loss_fun(out_fr, label_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    \n",
    "    # lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr/num_steps  \n",
    "\n",
    "            loss = loss_fun(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "\n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized:  flat\n",
      "Quantized:  linear1\n",
      "Quantized:  lif1\n",
      "Quantized:  linear2\n",
      "Quantized:  lif2\n",
      "Quantization time: 0.0009870529174804688\n",
      "Constructing axons from linear Layer\n",
      "Input layer shape(infeature, outfeature): [ 1 28 28] 1000\n",
      "Numer of neurons: 0, number of axons: 784\n",
      "Constructing neurons from linear Layer\n",
      "Hidden layer shape(infeature, outfeature):  (1000,) 10\n",
      "Numer of neurons: 1000, number of axons: 784\n"
     ]
    }
   ],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network\n",
    "from hs_api.api import CRI_network\n",
    "# import hs_bridge #Uncomment when running on FPGA\n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 1 #first pytorch layer that acts as synapses\n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(qn.v_threshold)\n",
    "    \n",
    "# #Have to be deployed on the FPGA\n",
    "# hardwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "#                               connections=dict(cn.neuron_dict),\n",
    "#                               config=config,target='CRI', \n",
    "#                               outputs = cn.output_neurons,\n",
    "#                               coreID=1)\n",
    "\n",
    "softwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,target='simpleSim', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "run_sw and run_hw are two helper functions for running the spiking neural network on HiAER-Spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cri_convert.bias_start_idx = int(cri_convert.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "RUN_HARDWARE = False #set to True if running on FPGA\n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cri_convert.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
