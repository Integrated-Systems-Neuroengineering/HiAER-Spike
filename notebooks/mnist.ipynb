{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "### **Feedforward Fully Connected SNN**\n",
    "\n",
    "This tutorial goes over how to train a simple feedforward SNN and deploy on HiAER Spike using our \n",
    "conversion pipline.\n",
    "\n",
    "### **Define a Feedforward SNN**\n",
    "To build a simple feedforward spiking neural network with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1238a-bb88-4a0e-9250-2ab4656dfaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spikingjelly\n",
    "!pip install torchvision torchviz torchview\n",
    "!pip install snntorch\n",
    "\n",
    "# Library to interact with the CRI hardware\n",
    "!pip install hs_api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a simple 2-layer feedforward SNN model with 1000 hidden neurons. The PyTorch layer will act as synapses between the spiking neuron layers. \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, features = 1000): \n",
    "        super().__init__() \n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28 * 28, features, bias=False) \n",
    "        self.lif1 = neuron.LIFNode(surrogate_function=surrogate.ATan()) \n",
    "        self.linear2 = nn.Linear(features, 10, bias=False) \n",
    "        self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan()) \n",
    "    def forward(self, x): \n",
    "        x = self.flat(x)\n",
    "        x = self.linear1(x) \n",
    "        x = self.lif1(x) \n",
    "        x = self.linear2(x) \n",
    "        x = self.lif2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Network\n",
    "net = model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd241796-eebd-4400-8dc0-8d1eb177a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 22712042.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 13438617.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 13012686.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3132280.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download MNIST data from torch \n",
    "mnist_train = datasets.MNIST('data/mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "mnist_test = datasets.MNIST('data/mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d18ffc1a",
   "metadata": {},
   "source": [
    "### [Optional] : The following is for Apple Silicon (M1/M2) users only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0569df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 20\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 5\n",
    "dtype = torch.float\n",
    "\n",
    "# Use Apple metal accelerator for training\n",
    "# device = torch.device(\"mps\") \n",
    "# Uncomment the following if you are not on apple silicon\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, train_loss = 0.0186, train_acc = 0.8900, test_loss = 0.0083, test_acc = 0.9546\n",
      "train speed = 1089.6638 images/s, test speed = 2819.4406 images/s\n",
      "epoch = 1, train_loss = 0.0065, train_acc = 0.9666, test_loss = 0.0056, test_acc = 0.9693\n",
      "train speed = 1140.4866 images/s, test speed = 2696.7095 images/s\n",
      "epoch = 2, train_loss = 0.0045, train_acc = 0.9778, test_loss = 0.0047, test_acc = 0.9756\n",
      "train speed = 1234.8581 images/s, test speed = 2934.2681 images/s\n",
      "epoch = 3, train_loss = 0.0034, train_acc = 0.9839, test_loss = 0.0039, test_acc = 0.9781\n",
      "train speed = 1177.7316 images/s, test speed = 2919.8789 images/s\n",
      "epoch = 4, train_loss = 0.0027, train_acc = 0.9883, test_loss = 0.0037, test_acc = 0.9825\n",
      "train speed = 1180.8772 images/s, test speed = 2791.8080 images/s\n",
      "epoch = 5, train_loss = 0.0022, train_acc = 0.9915, test_loss = 0.0035, test_acc = 0.9802\n",
      "train speed = 1108.6670 images/s, test speed = 2753.5173 images/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m train_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      6\u001b[0m train_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m img, label \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(img\u001b[39m.\u001b[39;49mnumpy(), mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/PIL/Image.py:3112\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3109\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3110\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[0;32m-> 3112\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/PIL/Image.py:3019\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3017\u001b[0m     args \u001b[39m=\u001b[39m mode, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[1;32m   3018\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _MAPMODES:\n\u001b[0;32m-> 3019\u001b[0m     im \u001b[39m=\u001b[39m new(mode, (\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m   3020\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39mmap_buffer(data, size, decoder_name, \u001b[39m0\u001b[39m, args))\n\u001b[1;32m   3021\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cri/lib/python3.10/site-packages/PIL/Image.py:2933\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2931\u001b[0m     im\u001b[39m.\u001b[39mpalette \u001b[39m=\u001b[39m ImagePalette\u001b[39m.\u001b[39mImagePalette()\n\u001b[1;32m   2932\u001b[0m     color \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mgetcolor(color)\n\u001b[0;32m-> 2933\u001b[0m \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39;49mfill(mode, size, color))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(num_steps):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_fr = out_fr/num_steps  \n",
    "        loss = loss_fun(out_fr, label_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr/num_steps  \n",
    "\n",
    "            loss = loss_fun(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "\n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hs_api.converter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhs_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverter\u001b[39;00m \u001b[39mimport\u001b[39;00m CRI_Converter, Quantize_Network\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhs_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m CRI_network\n\u001b[1;32m      3\u001b[0m \u001b[39m# Uncomment the following when running on FPGA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# import hs_bridge \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m#Weight, Bias Quantization \u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hs_api.converter'"
     ]
    }
   ],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network\n",
    "from hs_api.api import CRI_network\n",
    "# Uncomment the following when running on FPGA\n",
    "# import hs_bridge \n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 1 #first pytorch layer that acts as synapses\n",
    "output_layer = 3 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(qn.v_threshold)\n",
    "\n",
    "#Create a network running on the FPGA\n",
    "hardwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='CRI', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)\n",
    "\n",
    "#Create a network running on the software simulation\n",
    "softwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,\n",
    "                              target='simpleSim', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "Using the run_CRI_hw and run_CRI_sw method from the CRI_Converter class, we can deploy the converted SNN on the HiAER Spike platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "cn.bias_start_idx = int(cn.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "#set to True if running on FPGA\n",
    "RUN_HARDWARE = False \n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cn.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(cn.run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(cn.run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
